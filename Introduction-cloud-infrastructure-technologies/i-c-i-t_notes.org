* Cloud computing
is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction.
- IaaS
- PaaS
- SaaS
** Features
- Speed and Agility
- Cost
- Easy access to resources
- Maintenance
- Multi-tenancy
- Reliability
** Types
- Private cloud (OpenStack)
- Public cloud (AWS/Google)
- Hybrid cloud
* Virtualization
- can be offered on different hardware and software layers, like CPU (Central Processing Unit), Disk, Memory, Filesystems, etc. In this chapter, we will look at some examples of creating Virtual Machines (VMs) after emulating the different kind of hardware to install a Guest OS on that.
- Virtual Machines are created on top of a *Hypervisor*, which runs on top of the Host Machine's Operating System.
  - emulate hardware like CPU, Disk, Network, Memory and install guest machines
  - create multiple guest machines
  - examples
    - KVM
    - Xen
    - VMware
    - Virtualbox
    - Hyper-V
- *Nested virtualization*
** KVM 
- (Kernel-based Virtual Machine) is  a full virtualization solution for Linux on x86 hardware. Linux, Windows, Solaris
- It is part of the main-line Linux Kernel. It is ported for S/390, PowerPC, IA-64 and ARM as well.
- How it looks
[[file:./Kernel-based_Virtual_Machine.png]]
- KVM does not perform any emulation itself, but it exposes the */dev/kvm* interface, by which an external userspace host can do emulation.
- *QEMU* is one such host
- Advantages
  - open source
  - cheap
  - scalable
  - security features (SELinux). MAC(mandatory access control)
** VirtualBox
- an x86 and AMD64/Intel64 virtualization product from Oracle, which runs on Windows, Linux, Macintosh, and Solaris
- Windows, Linux families, and others, like Solaris, FreeBSD, DOS
- easy-to-use multi-platform Hypervisor
- not part of the mainline kernel.
- to use it on Linux, we have to compile and insert the respective kernel module.
- GNU General Public License (GPL) version 2.
** Vagrant
- Vagrant by Hashicorp helps us automate the setup of one or more VMs by providing an end-to-end life-cycle using the vagrant command line.
- Linux, Mac-OSX, and Windows.
- recent docker support
- Terms
  - Boxes                                    Base images
  - Vagrant Providers                        underlying engine/hypervisor used to provision a machine. Vagrant supports VirtualBox, Hyper-V and Docker.
  - Synced Folders
  - Provisioning                             allow us to automatically install software, make configuration changes,after machine boot. *File, Shell, Ansible, Puppet, Chef, Docker*
  - Plugins                                  to extend functionality
* IaaS
- form of Cloud Computing which provides on-demand physical and virtual computing resources, storage, network, firewall, load balancers, etc.
** Providers
- AWS : uses mostly the XEN Hypervisor.
  - examples
    - t2.nano: 512 MiB of memory, 1 vCPU, 3 CPU Credits/hour, EBS-only, 32 bit or 64-bit platform.
    - c4.large: 3.75 GiB of memory, 2 vCPUs, 64-bit platform.
    - d2.8xlarge: 244 GiB of memory, 36 vCPUs, 24 x 2000 GB of HDD-based instance storage, 64-bit platform, 10 Gigabit Ethernet.
  - pre-configured images, called Amazon Machine Images (AMIs).
  - Amazon supports configuring security and network access to our instances.
  - EBS can be attached or deattached
  - supports provisioning dedicated hosts
  - other features
    - Elastic IP
    - VPC
    - CloudWatch
    - Auto Scaling
- Azure
  - lets you choose between different tiers, based on the usage and the Operating Systems or the pre-defined application Virtual Machines
  - Using Resource Manager templates, we can define the template for the Virtual Machine Deployment.
- DigitalOcean
  - create a simple cloud quickly, in as little as 55 seconds.
  - All of the VMs are created on top of the KVM Hypervisor and have SSD (Solid-State Drive) as the primary disk.
  - Some features
  - Floating IPs within the same datacenter,
  - Shared Private Networking and
  - Team Accounts
- GCE
  - Persistent Disk, Local SSD, Global Load Balancing, Compliance & Security, Automatic Discount, etc.
  - It is very secure, encrypting all data stored.
  - supports custom machine types
- OpenStack
  - public and private clouds
  - floating ips
  - modular nature
  - components
    - Keystone : Provides Identity, Token, Catalog, and Policy services to projects.
    - Nova
      - select underlying Hypervisior
        - libvirt (qemu/KVM), Hyper-V, VMware, XenServer, Xen via libvirt.
    - Horizon
    - Glance
    - Swift
    - cinder
    - heat
    - Ceilometer
* PaaS
- allows its users to develop, run, and manage applications without worrying about the underlying infrastructure.
- from Cloud computing
  - Amazon
  - GCE
  - Azure
- on-premise
  - OpenShift Origin
  - Deis
** cloudfoundry
- can deployed on-premise or on IaaS, like AWS, vSphere, or OpenStack.
- commericial providers
  - HPE Helion Cloud Foundry, IBM Bluemix, Pivotal Cloud Foundry
- features
  - Application portability
  - Application auto-scaling
  - Centralized platform management
  - Dynamic routing
  - Role-based application deployment
  - Horizontal and vertical scaling
  - Application health management
  - Centralized logging
  - Security
  - Support for different IaaS technologies.
- subsytems
  - Bosh
    - component vms   :    run all the different components of CF to provide different PaaS functionalities.
    - application vms :    run diego containers (earlier garden)
    - *Bosh provides the system orchestration to configure VMs into well-defined state-through-manifest files*
  - cloud controller
    - runs the applications and other processes on provisioned VMs
  - Router
    - routes incoming traffic to cloud controller or to the application
- buildpacks
  - language specific
  - information about how to download dependencies and configure specific applications
  - custom buildpacks can be created
  - Droplet execution agent (dea)
- *droplet* contains an OS-specific pre-built root-filesystem called stack
- CF also supports running Docker images, but it uses the Garden runtime to run them
- [[file:./cf-arch.png]]
- [[https://courses.edx.org/courses/course-v1:LinuxFoundationX%2BLFS151.x%2B2T2016/courseware/090fb08a605c483a9dea2b714d084013/88510a1c669740f7b2fd98781f7efb68/?child%3Dfirst][for details]]
- free account [[run.pivotal.io]]
- OpenShift

** OpenShift
- by RedHat
- OpenShift v3, is built on top of the container technology, which uses Docker and Kubernetes
- can be deployed on Linux OS or microos (Atomic Host and its variants)
- Plans
  - Online     : pay as per usage 
  - dedicated  : managed by RedHat
  - enterprise : on your own hardware
- OpenShift origin
- Docker and K8S
- *Source to Image (S2I)*, which enables us to create Docker images from the source code repository to deploy applications easily.
- CI/CD integration
- Dedicated or Enterprise plans you get a GUI to manage projects, users, access, etc.
- We can look at the logs. OpenShift creates an internal Docker registry in which it pushes the images created via "s2i" process.
** Heroku
- container based cloud platform
- *Heroku Platform*
- Workflow
  - Procfile
  - buildpacks
  - runtime is called *slug*
  - third party add-ons to get access to value-added services like logging, caching, monitoring, etc.
  - slug + configuration variables + add-ons = release
  - Unix container is called *Dyno*
  - *Dyno manager*
  - *Salesforce integration*
** Deis
- docker and CoreOS
[[file:./deis-arch.png]]

- components
  - Control plane
    - management tasks
    - container scheduling
    - blob storage called *Store* has docker images, platform state, logs from the data plane
  - Data Plane
    - where all containers run
  - router mesh
- 100% open source
- supports Heroku buildpacks
[[file:./deis-workflow.png]]

* Conrainers
- Operating-System-level virtualization allows us to run multiple isolated user-space instances in parallel.
- user-space instances are *containers*
- When a container is created from an image, it runs as a process on the host's kernel.
- It is the host kernel's job to isolate and provide resources to each container.
** Building blocks
- Namespaces
  - pid
  - net
  - ipc
  - uts
  - mnt
  - user
- cgroups: organize processes hierarchically and distribute system resources along the hierarchy in a controlled and configurable manner.
  - blkio
  - cpu
  - cpuacct
  - cpuset
  - devices
  - freezer
  - memory
- *ufs* :
  - allows files and directories of separate filesystems, known as layers, to be transparently overlaid on each other, to create a new virtual filesystem.
  - An image used in Docker is made of multiple layers and, while starting a new container, we merge all those layers to create a read-only filesystem.
  - On top of a read-only filesystem, a container gets a read-write layer, which is an ephemeral layer and it is local to the container.
** Container runtimes
- *runC* is the CLI tool for spawning and running containers according to the specifications (The Open Container Initiative).
- Docker
  - [[file:./docker-engine.png]]
- rkt coreOS
* Micro OSes for containers
** Atomic Host
  - RPM content
  - can be based on Fedora, CentOS, or Red Hat Enterprise Linux (RHEL).
  - components
    - rpm-ostree : manage 2 FS
    - systemd : manage services
    - Docker
    - K8S
  - Cockpit
  - features
    - increased security through SELinux.
    - can be installed on bare-metal, as well as VMs.
    - can be based on Fedora, CentOS, and Red Hat Enterprise Linux.
    - Nodes can be clustered on Atomic Host using Kubernetes.
** CoreOS
  - arch [[file:./core-os-arch.png]]
  - Amazon EC2, Digital Ocean, Microsoft Azure, Google Cloud,
  - bare metal or vms
  - *cloud-config*
  - features
    - no package manager
    - active / passive root partitions
    - when running with active, passive can be used to download updates
    - *self-update* feature
    - specific release channels with update strategies can be chosen
    - High Availability and security
    - most cloud providers
    - Docker and rkt
    - Paid support
    - update without downtime
  - components
    - docker
    - etcd
    - systemd
    - fleet : launch applications using systemd
  - enterprise the Kubernetes solution called Tectonic.
** VMWare Photon
- minimal
- small footprint
- quickly boot
- *optimized for VMWare products*
- supports Docker, rkt, and the Pivotal Garden
- open-source, yum-compatible package manager (tdnf)
- Amazon EC2, Google Cloud, and Microsoft Azure
** Rancher OS
- least footprint
- arc : [[file:./rancher-os-arch.png]]
- 
* Container orchestration
** problems to be solved
- Who can bring multiple hosts together and make them part of a cluster ?
- schedule the containers to run on specific hosts
- containers to communicate with each irrespective of on which host it is run
- Who will make sure that the container has the dependent storage, when it is scheduled on a specific host ?
- Who will make sure that containers are accessible over a service name, so that we do not have to bother about container accessibility over IP addresses?
** Docker Swarm
- [[file:./swarm-arch.jpg]]
- multiple docker engines to create a virtual engine
- components
  - Swarm manager (active/passive) : accepts command on behalf of the cluster
  - agents : hosts that run docker engines
  - discovery service : libkv abstracts KV like etcd, zookeeper, consul
  - Overlay Networking : libnetwork
- filters : node, container
- strategies : spread, binpack, random
- pluggable scheduler architecture : mesos and k8s can be plugged
- *docker machine*
  - drivers for Amazon EC2, Google Cloud, Digital Ocean, Vagrant
** K8S
- [[file:./k8s-arch.png]]
- multiple volume plugins like the GCP/AWS disk, NFS, iSCSI, Ceph, Gluster, Cinder, Flocker, etc. to attach volumes to Pods.
- automatically self heal
- batch execution
** Apache mesos
- Apache Mesos was created with this in mind, so that we can optimally use the resources available, even if we are running disparate applications on a pool of nodes.
- cluster of nodes as one big computer, which manages CPU, memory, and other resources across a cluster.
- [[file:./mesos-arch.jpg]]
- *Marathon* is used to schedule
- components
  - master
  - slaves
  - frameworks
  - executors
- features
  - uses zookeeper
  - enables native isolation between tasks with Linux Containers
  - Java, Python and C++ APIs
  - WebUI
** Nomad
- from hashicorp
- single binary
- client and server mode
- HCL
- cluster management and resource scheduling
** Cloud foundry diego
** Amazon ECS
- [[file:./ecs-arch.png]]
- components
  - cluster
  - container instances
  - task definition
  - Schduler
    - Service schduler
    - RunTask
    - StartTask
    - Service
    - Task
    - Container
- features
  - has provisions to integrate with third party schedulers, like *Mesos Marathon*
  - Amazon ECS CLI is compatible with Docker Compose.
  - AWS CloudWatch
  - AWS CloudTrail
  - Docker Registry or Docker Hub.
** Google container engine
- running K8S on google cloud
- features
  - hybrid networking
** Azure container service
- apache mesos or docker swarm
- templates for both Mesos and Docker Swarm
* Unikernels
- select the part of the kernel needed to run with the specific application.
- single address space executable, which has both application and kernel components.
- deployed on VMs or bare-metal.
- specialized Virtual Machine images with
  - application code
  - configuration files
  - user space libraries needed by the application
  - application runtime
  - system libs which allow inter communication with hypervisior
- [[file:./unikernel-vs-os.png]]
- A more secured application than the traditional VM, as the attack surface is reduced.
- Implementations
  - Special purpose built        : ING, HalVM, MirageOS, and Clive.
  - Generalized 'fat' unikernels : BSD Rump kernels, OSv, Drawbridge
- Unikernels helped Docker to run the Docker Engine on top of Alpine Linux on Mac and Windows with their default hypervisors,
- which are xhyve Virtual Machine and Hyper-V VM respectively.
- [[file:./shared-vs-unikernel.png]]

* microservices
* Container as a service (CaaS)
** solutions
- OpenStack Magnum
- Docker Universal Control Plane
** CaaS enablers
- K8S
- AWS EC2 container service (ECS)
- Tectonic (K8S + CoreOS)
- Rancher
** Docker UCP
- on-premise or virtual
- [[file:./docker-caas-ecosystem.jpg]]
- features
  - works with swarm and docker machine
  - LDAP/AD
  - web admin GUI
** Docker Datacenter
- built on top of UCP and Docker Trusted Registry
- hosted completely behind firewall
- enterprise-class CaaS platform on-premises
** OpenStack Magnum
- Containers are orchestrated and scheduled on clusters created by Kubernetes, Docker Swarm, or Mesos.
- [[file:./magnum-arch.png]]
- components
  - Server API  : magnum client talks to the service
  - Conductor   : It manages the cluster lifecycle through Heat and communicates with the Cluster Container Orchestration Engine (COE)
  - Bays        : are nodes on which the Container Orchestration Engine sets up the respective Cluster.
  - BayModels   : stores metadata information about Bays like the Container Orchestration Engine, Keypairs, Images to use, etc.
  - COE         : supports Kubernetes, Docker Swarm, and Mesos. COEs are on top of Micro OSes, like Atomic Host, CoreOS, etc.
  - Pod
  - Service                 : logical set of pods
  - Replication Controller  :
  - Container
- Magnum offers an asynchronous API that is compatible with Keystone.
- It brings multi-tenancy by default, as it creates a Bay per tenant.

* SDN
- decouples the network control layer from the layer which forwards the traffic.
- 3 Planes
  - Data Plane       : for handling data packets and apply actions to them based on rules which we program into lookup-tables.
  - Control Plane    : calculating and programming the actions for the Data Plane. 
  - Management Plane : we can configure, monitor, and manage the network devices.
- In SDN
  - we decouple the Control Plane with the Data Plane.
  - The Control Plane has a centralized view of the overall network, which allows it to create forwarding tables of interest.
  - These tables are then given to the Data Plane to manage network traffic. 
  - [[file:./sdn-fwk.png]]
- We can use configuration tools like Ansible or Chef to configure SDN

* Networking for containers
- host kernel uses the Network Namespace feature of the Linux Kernel to isolate the network from one container to another.
- Network Namespaces can be shared as well.
- On a single host, when using the *Virtual Ethernet (veth)* feature with Linux bridging, we can give a virtual network interface to each container and assign it an IP address.
- With Linux Kernel features, like *IPVLAN*, we can configure each container to have a unique and world-wide routable IP address.
- multi-host networking with containers
  - some form of Overlay network driver
  - encapsulates Layer 2 traffic to a higher layer
  - e.g. Docker Overlay Driver, Flannel, Weave, etc. Project Calico allows multi-host networking on Layer 3 using BGP (Border Gateway Protocol).
- Container Networking standards
  - Container Network Model (CNM)
    - Docker, Inc. is the primary driver for this networking model. It is implemented using the libnetwork project
    - Null - NOOP implementation of the driver. It is used when no networking is required.
    - Bridge - It provides a Linux-specific bridging implementation based in Linux Bridge.
    - Overlay - It provides a multi-host communication over VXLAN.
    - Remote - It does not provide a driver. Instead, it provides a means of supporting drivers over a remote transport, by which we can write third-party drivers.
  - Container Networking Interface (CNI) Proposal
    - CoreOS
    - derived from rkt Networking Proposal
    - Kubernetes has added support for CNI.
* Service Discovery
- Registration
- Lookup
  - SkyDNS and Mesos-DNS are examples 
* docker single host networking
- bridge
  - can forward traffic between two networks based on MAC
  - Docker uses the Linux's Virtual Ethernet (veth) feature to create two virtual interfaces
    - attached to container
    - other end to the docker0 bridge
- Null
  - no networking
  - only loopback
  - no access from outside
- Host
  - we can share the host machine's network namespace with a container.
  - container has full access to host's network
* Sharing Network Namespaces among Containers
- two or more containers can have the same network stack and reach each other by referring to localhost.
- ~--net=container:CONTAINER~
- Kubernetes uses the feature detailed above to share the same network namespaces among multiple containers in a pod.
* docker multi-host networking
- Overlay network,
  - in which we encapsulate the container's IP packet inside the host's packet, while sending it over the wire.
  - While receiving, we decapsulate the whole packet and forward the container's packet to the receiving container.
- examples based on overlay network
  - Docker Overlay Driver
  - Flannel
  - Weave
- Project Calico uses the Border Gateway Protocol (BGP) to do IP-based routing instead of encapsulation.
  - As it works on Layer 3, it requires some hardware support.
** Docker Overlay driver
- works based on "libnetwork, a built-in VXLAN-based overlay network driver, and Dockerâ€™s libkv library".
- [[file:./docker-overlay-nw.png]]
* docker networking plugins
- weave network plugin
  - multi-host container networking for Docker
  - provides service discovery and does not require any external cluster store to save networking configuration.
- Kuryr Network Plugin
  - part of the OpenStack Kuryr project, which also implements Libnetwork's remote driver API by utilizing Neutron, which is OpenStack's networking service. 
* Software Defined Storge (SDS)
- [[file:./sds.png]]
- some examples
  - Ceph
  - Gluster
  - FreeNAS
  - Nexenta
  - VMWarw Virtual SAN
** Ceph
- Ceph is a distributed object store and file system designed to provide excellent performance, reliability and scalability.
- [[file:./ceph-arch.png]]
- RADOS : It is the object store which stores the objects.
  - Object Storage Device
  - Ceph Monitors (MON)
  - Ceph Metadata Server (MDS)
- Benifits
  - It is an Open Source storage solution, which supports Object, Block, and Filesystem storage.
  - It runs on any commodity hardware, without any vendor lock-in.
  - It provides data safety for mission-critical applications.
  - It provides automatic balance of filesystems for maximum performance.
  - It is scalable and highly available, with no single point of failure.
  - It is a reliable, flexible, and cost-effective storage solution.
** Gluster
- GlusterFS is a scalable network filesystem.
- storage solutions for media streaming, data analysis, and other data- and bandwidth-intensive tasks.
- free and open source software.
- to create
  - grouping the machines in a Trusted Pool
  - we group the directories (called bricks) from those machines in a GlusterFS volume, using FUSE (Filesystem in Userspace)
  - supported volume kinds
    - Distributed GlusterFS Volume
    - Replicated GlusterFS Volume
    - Distributed Replicated GlusterFS Volume
    - Striped GlusterFS Volume
    - Distributed Striped GlusterFS Volume.
- does not have a centralized metadata server. It uses an elastic hashing algorithm to store files on bricks.
- Access
  - Native FUSE mount
  - NFS
  - CIFS (Common Internet File System).
- [[file:./gluster-fs-volume.png]]
- Benifits
  - It is an Open Source storage solution that supports Object, Block, and Filesystem storage.
  - It does not have a metadata server.
  - It is scalable, modular and extensible.
  - It is POSIX-compliant, providing High Availability via local and remote data replication.
* Docker Storage backends
- Docker uses Copy-on-write to start containers from images
- supported
  - AUFS
  - BtrFS
  - Device Mapper
  - VFS
  - ZFS
* Docker Volumes
- data volume is a specially-designated directory within one or more containers that bypasses the Union File System.
- *Data Volume Container* just creates a volume
* Volume Plugins for Docker
- Flocker
  - Flocker is an open-source container data volume manager for your Dockerized applications.
  - Flocker Docker Volume is referred to as a dataset, which can be used with any of the containers in the cluster.
  - [[file:./flocker.jpg]]
  - Supported storage options
    - AWS Elastic Block Storage (EBS)
    - OpenStack Cinder with any supported backend
    - EMC ScaleIO, XtremeIO, VMAX
    - VMware vSphere and vSan
    - NetApp OnTap. 
- GlusterFS
- Blockbridge
- EMC Rex-Ray
* DevOps and CI/CD
** Jenkins
- CloudBees
- build Freestyle, Apache Ant, and Apache Maven-based projects
- plugins to extend functionlity
- Pipeline : entire application lifecycle
  - durable
  - pausable
  - versatile
  - extensible
- on premise
- cloud
- as a SaaS
** Drone
- only uses docker
- hosted and on-prem
- CI/CD
- Github and Bitbucket
- Deploying
  - Heroku
  - AppEngine
  - dotCloud
  - SSH
  - Amazon S3
- free for public projects.
- integrated with GitHub, Bitbucket, and Google Code,
- Docker containers to test code
** Travis CI
- hosted, distributed CI solution for projects hosted only on GitHub.
- link our GitHub account with Travis and select the project (repository) for which we want to run the test.
- .travis.yml file, which defines how our build should be executed step-by-step.
- steps
  - before_install
  - install : dependencies
  - before_script
  - script  : run build script
  - after_success or after_failure
  - Optional before_deploy
  - Optional deploy
  - Optional after_deploy
  - after_script
- deployment to ~Heroku, AWS Codedeploy, Cloud Foundry, OpenShift~
- feature
  - supports testing for different versions of the same runtime.
  - free and open source projects
  - hosted, distributed integrated with Github
** Shippable
- CI and CD pipeline for Github/ Bitbucket and other repositories
- runs all the builds inside a Docker container, which are called minions.
- [[file:./shippable-struct.png]]
- *deploy the application on container services like Amazon ECS and Google Container Engine*
- supports on-premise systems for builds.
* Tools for Cloud infratructure
- Infratructure as a code
- Some tools
  - Ansible
  - Chef
  - Puppet
  - Salt
** Ansible
- Red hat
- open source
- agentless tool which works on top of SSH
- also automates cloud provisioning, application deployment, orchestration
- dynamic inventory files for cloud providers like AWS and OpenStack
- Playbooks
  - configuration, deployment, and orchestration language
- The Ansible Management node connects to nodes mentioned in the inventory file and runs the tasks mentioned in the playbook.
- A Management node can be installed on any *nix-based system like Linux, Mac OS X
- can manage any node which supports SSH and Python 2.4 or later
- Ansible Galaxy is a free site for finding, downloading, and sharing community-developed Ansible roles.
- Ansible Tower: enterprise with GUI
- agentless tool
- low learning curve
** Puppet
- Puppet agent/ Puppet master (client/server) model to configure the systems.
- Puppet enterprise
- install Puppet Agent on each system
- each *agent*
  - Connects securely to Puppet Master to get the series of instructions in a file referred to as the Catalog File.
  - Performs operations from the Catalog File to get to the desired state.
  - Sends back the status to Puppet Master.
  - can be installed on Linux, Mac OSX
- *Puppet Master* can be installed only on *nix systems.
  - Compiles the Catalog File for hosts based on the system, configuration, manifest file, etc.
  - Sends the Catalog File file to agents when they query the master.
  - Has information about the entire environment, such as host information, metadata like authentication keys, etc.
  - Gathers the report from each agent and then prepares the overall report.
- Puppet defines resources on a system as Type which can be file, user, package, service, etc.
- Nice features
  - Centralized reporting (PuppetDB)
  - Live system management
  - Puppet Forge, which has ready-to-use modules for manifest files from the community.
  - role-based access control.
** Chef
- client/server model
- Chef client
- Chef server
- Chef workstation
  - Develop cookbooks and recipes.
  - Synchronize chef-repo with the version control system.
  - run command line tools
  - Interact with nodes to do a one-off configuration.
- [[file:./chef-overview.png]]
- Chef cookbook
  - Recipes
  - Attributes
- Knife provides an interface between a local chef-repo and the Chef Server.
- Chef client supported on
  - *nix-bases
  - Mac OSX
  - Windows
  - Cisco IOS XR and NX-OS
- Chef server
  - RHEL
  - Ubuntu
- features
  - It provides role-based access control.
  - It provides real-time visibility with Chef Analytics.
** Salt
- [[file:./salt-minion.png]]
- remote execution framework
- minions
  - *nix
  - osx
  - windows
- Salt Master and Minions communicate over a high speed data bus, ZeroMQ,
- Salt also supports an agentless setup on top of SSH.
- The Salt Master and Minions communicate over a secure and encrypted channel.
- components
  - module      : installing packages, managing files, managing containers,
  - returners   : save a Minion's response on the Master or other locations
  - grains      : All the information collected from Minions is saved on the Master. The collected information is referred to as Grains
  - pillars data:
    - Private information like cryptographic keys and other specific information about each minion which the Master has is referred to as Pillar Data.
    - Pillar Data is shared between the Master and the individual Minion.
- features
  - agentless
  - rbac

